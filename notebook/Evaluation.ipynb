{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c6acdcc",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d87d702",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics.image import PeakSignalNoiseRatio, StructuralSimilarityIndexMeasure\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import pytorch_lightning as pl\n",
    "from torchvision import models\n",
    "import os\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbf804e",
   "metadata": {},
   "source": [
    "# Data Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d5613c",
   "metadata": {},
   "source": [
    "## Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "33edd558",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoLDataset(Dataset):\n",
    "    def __init__(self, dark_dir, bright_dir, transform):\n",
    "        self.dark_dir = dark_dir\n",
    "        self.bright_dir = bright_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Read and sort both folders independently\n",
    "        self.dark_images = sorted([f for f in os.listdir(dark_dir) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "        self.bright_images = sorted([f for f in os.listdir(bright_dir) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "        \n",
    "        # Quick safety print if there's a mismatch in folder sizes\n",
    "        if len(self.dark_images) != len(self.bright_images):\n",
    "            print(f\"WARNING: Found {len(self.dark_images)} dark but {len(self.bright_images)} bright in {dark_dir.split('/')[-2]}\")\n",
    "\n",
    "    def __len__(self): \n",
    "        # Safely return the smaller count so it never asks for an index that doesn't exist\n",
    "        return min(len(self.dark_images), len(self.bright_images))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Uses the specific filename found in each respective folder\n",
    "        dark_path = os.path.join(self.dark_dir, self.dark_images[idx])\n",
    "        bright_path = os.path.join(self.bright_dir, self.bright_images[idx])\n",
    "        \n",
    "        dark_img = Image.open(dark_path).convert(\"RGB\")\n",
    "        bright_img = Image.open(bright_path).convert(\"RGB\")\n",
    "        \n",
    "        return self.transform(dark_img), self.transform(bright_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aab82eb",
   "metadata": {},
   "source": [
    "## Lightning DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b75f97a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoLDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, base_path, batch_size=8):\n",
    "        super().__init__()\n",
    "        # Pointing to the specific subfolders from your image\n",
    "        self.train_low = os.path.join(base_path, \"Real_captured/Train/Low\")\n",
    "        self.train_high = os.path.join(base_path, \"Real_captured/Train/Normal\")\n",
    "        self.val_low = os.path.join(base_path, \"Real_captured/Test/Low\")\n",
    "        self.val_high = os.path.join(base_path, \"Real_captured/Test/Normal\")\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Training set \n",
    "        self.train_ds = LoLDataset(self.train_low, self.train_high, self.transform)\n",
    "        # Validation set\n",
    "        self.val_ds = LoLDataset(self.val_low, self.val_high, self.transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_ds, batch_size=self.batch_size, shuffle=True, num_workers=4, persistent_workers=True, pin_memory=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_ds, batch_size=self.batch_size, shuffle=False, num_workers=4, persistent_workers=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dbe3d0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataModule...\n",
      "Total training pairs found: 689\n",
      "Total validation pairs found: 100\n",
      "Batch Shape (Dark): torch.Size([8, 3, 224, 224])\n",
      "Batch Shape (Bright): torch.Size([8, 3, 224, 224])\n",
      "Pixel Range: 0.00 to 0.60\n",
      "DataModule is ready to feed the model.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- TEST ---\n",
    "def test_datamodule():\n",
    "    print(\"Testing DataModule...\")\n",
    "    dm = LoLDataModule(base_path=\"/home/sanjeet/ai_workspace/Image Enhancement/lol v2 dataset\") \n",
    "    dm.setup()\n",
    "    \n",
    "    # Check Training Set\n",
    "    train_loader = dm.train_dataloader()\n",
    "    dark_batch, bright_batch = next(iter(train_loader))\n",
    "    \n",
    "    print(f\"Total training pairs found: {len(dm.train_ds)}\")\n",
    "    print(f\"Total validation pairs found: {len(dm.val_ds)}\")\n",
    "    print(f\"Batch Shape (Dark): {dark_batch.shape}\")\n",
    "    print(f\"Batch Shape (Bright): {bright_batch.shape}\")\n",
    "    \n",
    "    # Check if pixels are normalized (should be between 0 and 1)\n",
    "    print(f\"Pixel Range: {dark_batch.min():.2f} to {dark_batch.max():.2f}\")\n",
    "    print(\"DataModule is ready to feed the model.\\n\")\n",
    "\n",
    "test_datamodule()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9595fd",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "adf410f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NanoLILY(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NanoLILY, self).__init__()\n",
    "\n",
    "        # --- BRANCH A: THE ARTIST ---\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.InstanceNorm2d(32),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.dec1 = nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.final_spatial = nn.Conv2d(32, 3, kernel_size=3, padding=1)\n",
    "\n",
    "        # --- BRANCH B: THE SIEVE ---\n",
    "        # Initialize with SMALL values (0.01) so it doesn't overwhelm the image early on\n",
    "        self.freq_mask = nn.Parameter(torch.full((1, 3, 224, 113), 0.01))\n",
    "\n",
    "        # --- THE FUSION ---\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Conv2d(6, 3, kernel_size=1),\n",
    "            nn.Tanh() # Constrains the residual to a stable range [-1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        # Spatial Pass\n",
    "        s1 = self.enc1(x)\n",
    "        s2 = self.enc2(s1)\n",
    "        up1 = self.dec1(s2)\n",
    "        spatial_out = self.final_spatial(up1 + s1) \n",
    "\n",
    "        # Frequency Pass\n",
    "        x_fft = torch.fft.rfft2(x)\n",
    "        x_fft_filtered = x_fft * self.freq_mask\n",
    "        freq_out = torch.fft.irfft2(x_fft_filtered, s=(224, 224))\n",
    "\n",
    "        # Fusion\n",
    "        combined = torch.cat([spatial_out, freq_out], dim=1)\n",
    "        residual = self.fusion(combined)\n",
    "\n",
    "        return identity + residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a5349501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Output Shape: torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# --- TEST ---\n",
    "model = NanoLILY()\n",
    "sample_input = torch.randn(1, 3, 224, 224)\n",
    "output = model(sample_input)\n",
    "print(f\"Success! Output Shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0276995a",
   "metadata": {},
   "source": [
    "# The Perceptual Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1e86cee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGPerceptualLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # We only need the feature extraction layers\n",
    "        vgg = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1).features\n",
    "        # Slicing up to layer 18 (ReLU 4_2)\n",
    "        self.slice = nn.Sequential(*list(vgg.children())[:18]).eval()\n",
    "        # Freezing parameters so we don't train VGG\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        output_feat = self.slice(output)\n",
    "        target_feat = self.slice(target)\n",
    "        return F.mse_loss(output_feat, target_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262a7f0c",
   "metadata": {},
   "source": [
    "# The Training System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cfb6d8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NanoLILYSystem(pl.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c88c635",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650703ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting full metrics for 128k model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_70645/3963923193.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path_128k, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "üèÜ 128k MODEL DIAGNOSTIC METRICS üèÜ\n",
      "========================================\n",
      "       val_psnr: 19.19457\n",
      "       val_ssim: 0.82591\n",
      "  val_char_loss: 0.09114\n",
      "   val_vgg_loss: 0.66296\n",
      "   val_fft_loss: 0.01738\n",
      "    val_tv_loss: 0.05561\n",
      " val_color_loss: 0.01628\n",
      "  val_ssim_loss: 0.17409\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# --- 1. DEFINE THE MISSING LOSS FUNCTIONS ---\n",
    "def compute_charbonnier_loss(output, target, eps=1e-3):\n",
    "    diff = output - target\n",
    "    return torch.mean(torch.sqrt(diff * diff + eps * eps))\n",
    "\n",
    "def compute_color_loss(output, target):\n",
    "    return torch.mean(1.0 - F.cosine_similarity(output, target, dim=1))\n",
    "\n",
    "def compute_tv_loss(x):\n",
    "    tv_h = torch.mean(torch.abs(x[:, :, 1:, :] - x[:, :, :-1, :]))\n",
    "    tv_w = torch.mean(torch.abs(x[:, :, :, 1:] - x[:, :, :, :-1]))\n",
    "    return tv_h + tv_w\n",
    "\n",
    "# --- 2. SETUP DATA AND METRICS ---\n",
    "dm = LoLDataModule(base_path=\"../dataset/lol_v2\", batch_size=16)\n",
    "dm.setup()\n",
    "val_loader = dm.val_dataloader()\n",
    "\n",
    "psnr_metric = PeakSignalNoiseRatio(data_range=1.0).cuda()\n",
    "ssim_metric = StructuralSimilarityIndexMeasure(data_range=1.0).cuda()\n",
    "vgg_loss_fn = VGGPerceptualLoss().cuda() \n",
    "\n",
    "# --- 3. LOAD MODEL ---\n",
    "model_128k = NanoLILY() \n",
    "\n",
    "\n",
    "ckpt_path_128k = \"../model/NanoLILY_best_weights.ckpt\" \n",
    "checkpoint = torch.load(ckpt_path_128k, map_location=\"cpu\")\n",
    "\n",
    "# Handle standard checkpoint dictionary or raw weights\n",
    "if 'state_dict' in checkpoint:\n",
    "    state_dict = {k.replace('model.', ''): v for k, v in checkpoint['state_dict'].items()}\n",
    "    model_128k.load_state_dict(state_dict, strict=False)\n",
    "else:\n",
    "    model_128k.load_state_dict(checkpoint)\n",
    "\n",
    "model_128k.cuda().eval()\n",
    "\n",
    "# --- 4. EVALUATE ---\n",
    "total_psnr, total_ssim, total_vgg, total_fft = 0.0, 0.0, 0.0, 0.0\n",
    "total_char, total_color, total_tv = 0.0, 0.0, 0.0\n",
    "batches = 0\n",
    "\n",
    "print(\"Extracting full metrics for 128k model...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for dark, bright in val_loader:\n",
    "        dark, bright = dark.cuda(), bright.cuda()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model_128k(dark)\n",
    "        \n",
    "        # Calculate standard metrics\n",
    "        total_psnr += psnr_metric(output, bright).item()\n",
    "        \n",
    "        ssim_val = ssim_metric(output, bright).item()\n",
    "        total_ssim += ssim_val\n",
    "        \n",
    "        total_vgg += vgg_loss_fn(output, bright).item()\n",
    "        \n",
    "        # Calculate FFT\n",
    "        out_fft = torch.fft.rfft2(output.float(), norm=\"ortho\")\n",
    "        tar_fft = torch.fft.rfft2(bright.float(), norm=\"ortho\")\n",
    "        total_fft += F.l1_loss(torch.abs(out_fft), torch.abs(tar_fft)).item()\n",
    "        \n",
    "        # Calculate the missing losses\n",
    "        total_char += compute_charbonnier_loss(output, bright).item()\n",
    "        total_color += compute_color_loss(output, bright).item()\n",
    "        total_tv += compute_tv_loss(output).item()\n",
    "        \n",
    "        batches += 1\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"üèÜ 128k MODEL DIAGNOSTIC METRICS üèÜ\")\n",
    "print(\"=\"*40)\n",
    "print(f\"       val_psnr: {total_psnr / batches:.5f}\")\n",
    "print(f\"       val_ssim: {total_ssim / batches:.5f}\")\n",
    "print(f\"  val_char_loss: {total_char / batches:.5f}\")\n",
    "print(f\"   val_vgg_loss: {total_vgg / batches:.5f}\")\n",
    "print(f\"   val_fft_loss: {total_fft / batches:.5f}\")\n",
    "print(f\"    val_tv_loss: {total_tv / batches:.5f}\")\n",
    "print(f\" val_color_loss: {total_color / batches:.5f}\")\n",
    "print(f\"  val_ssim_loss: {1.0 - (total_ssim / batches):.5f}\")\n",
    "print(\"=\"*40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
